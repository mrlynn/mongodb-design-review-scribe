ğŸ“„ Product Requirements Document (PRD)
Real-time Conversational Research Companion
ğŸ“Œ Overview
Product Name:
Real-time Conversational Research Companion

Description:
A cross-platform desktop application that listens to real-time conversations (via microphone), transcribes speech to text, extracts topics/questions, and automatically performs research to surface relevant insights.
It leverages a local LLM (via Ollama) to keep user data private and performant.

Target Platforms:

Initial: macOS

Next: Windows & Linux (cross-platform via Electron or Tauri)

ğŸ¯ Goals & Objectives
Goal	Description
ğŸ¤ Passive listening	Continuously capture speech and transcribe in real time
ğŸ“ Topic extraction	Use local LLM to detect main topics, questions, or unknown terms
ğŸ” Automatic research	Fetch info from web (Google, Wikipedia, others) on detected topics
ğŸ–¥ï¸ Insight presentation	Show research summaries, links, and highlights in an intuitive GUI
ğŸ”§ Configurable stack	Allow switching LLM models, adjusting thresholds, enabling/disabling research
ğŸ”’ Local-first privacy	All speech & LLM processing is local; only web searches leave the machine

ğŸ§­ Scope
âœ… MVP
ğŸ™ Real-time mic input + transcription (via Whisper / whisper.cpp)

ğŸ” Local LLM (Ollama) for topic extraction

ğŸŒ Simple research module (search Google/Wikipedia for top topics)

ğŸ–¥ Basic GUI timeline to show:

transcript text

extracted topics

summaries from research

ğŸ’¾ Local persistence (store sessions, config, history)

âš™ Config file for:

LLM endpoint & model

speech-to-text settings

research providers

thresholds (chunk size, min topic confidence)

ğŸš« Not in MVP
Multi-language support

Meeting participant separation / diarization

OAuth for personal knowledge sources (Notion, Confluence, etc.)

Mobile app

ğŸ— Architecture Overview
csharp
Copy
Edit
[Microphone Input]
   â†“
[Speech-to-Text]
   â†“
[Chunking (every N words/seconds)]
   â†“
[Local LLM Topic Extraction]
   â†“
[Web Research]
   â†“
[UI Timeline & Insights Panel]
   â†“
[Local storage]
ğŸ›  Tech Stack
Layer	Tech Options
GUI	Electron + React / Tauri + React
Speech-to-text	whisper.cpp (via local CLI)
LLM	Ollama (local endpoint, configurable model)
Research	Node HTTP requests to Google Custom Search, Wikipedia
Data storage	JSON / LiteDB / SQLite (for sessions & config)
Config	JSON file in user home dir (~/.research-companion/config.json)

âš™ï¸ Config Example
json
Copy
Edit
{
  "llm": {
    "provider": "ollama",
    "model": "llama3",
    "endpoint": "http://localhost:11434"
  },
  "speech_to_text": {
    "method": "whisper.cpp",
    "language": "en"
  },
  "research": {
    "providers": ["google", "wikipedia"],
    "max_results": 3
  },
  "thresholds": {
    "min_words_per_chunk": 25
  }
}
ğŸš€ User Stories
As a ...	I want to ...	So that ...
Casual user	start the app and let it run in the background	it automatically gives me research without effort
Developer	configure which LLM or speech-to-text engine to use	I can test different local setups
Privacy-minded	keep everything local (speech & LLM processing)	none of my private conversations go to the cloud
Researcher	see topics & automatic summaries in a clean timeline	I can quickly learn about what's discussed

ğŸ“‹ Acceptance Criteria
âœ… Mic input works with acceptable real-time latency (< 3s lag)

âœ… LLM detects topics & outputs 1-3 main keywords/questions per chunk

âœ… Research module fetches at least 2 sources per topic

âœ… UI shows a clear timeline with: transcript, topics, summary

âœ… Config file can change LLM model, thresholds, research providers

âœ… No audio or transcription data is uploaded anywhere except direct web lookups

ğŸ“‚ Directory Structure
bash
Copy
Edit
/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.js          # Electron/Tauri entry
â”‚   â”œâ”€â”€ renderer/        # React UI
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ stt.js       # Speech-to-text integration
â”‚   â”‚   â”œâ”€â”€ llm.js       # Ollama interface
â”‚   â”‚   â”œâ”€â”€ research.js  # Web research handlers
â”‚   â”œâ”€â”€ storage/         # JSON / DB handling
â”‚   â””â”€â”€ config/          # Config loader & schema
â”œâ”€â”€ whisper/             # (or scripts to invoke whisper.cpp)
â””â”€â”€ README.md
ğŸ”¥ Milestones
Milestone	Description	Target Date
âœ… Prototype transcription	Mic â†’ Whisper â†’ Console output	+2 weeks
âœ… Integrate Ollama	Chunk text â†’ send to LLM â†’ get topics	+4 weeks
âœ… Build research fetcher	Google/Wikipedia â†’ get summaries	+6 weeks
âœ… UI timeline	Transcript + topics + summaries	+8 weeks
âœ… Config / local persistence	JSON file for settings & session saves	+10 weeks
âœ… Build Mac app bundle	Electron/Tauri packaged .app	+12 weeks

ğŸ“ Open Questions
Should we offer auto start on system boot for power users?

Should we allow live voice command triggers (e.g. â€œHey Companion, summarize thisâ€)?

Future integrations: local vector store to remember what user already learned.

ğŸ“Œ Ownership
Product Owner: You (or appointed PM)

Engineering Lead: TBD

Design: TBD

Testing: TBD

âœ… Done means:

App can run on macOS, start listening, summarize topics, and present research in a UI timeline, with configurable local LLM and local-only processing (except external research lookups).
